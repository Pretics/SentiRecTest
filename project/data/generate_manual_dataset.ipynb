{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual 버전 데이터셋 제작\n",
    "\n",
    "해당 코드는 기존의 데이터셋(demo/small/large)에서 설정한 갯수의 데이터만을 랜덤하게 가져와 새로운 사이즈(manual)의 데이터셋을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "경로가 폴더를 나타낼 경우 Dir, 파일일 경우 Path로 명명\n",
    "\"\"\"\n",
    "\n",
    "size = \"manual\"\n",
    "original_size = \"demo\"\n",
    "\n",
    "datasetDir = f\"MIND/{size}\"\n",
    "datasetTrainDir = f\"{datasetDir}/train\"\n",
    "datasetTestDir = f\"{datasetDir}/test\"\n",
    "\n",
    "datasetOriginalDir = f\"MIND/{original_size}\"\n",
    "datasetOriginalTrainDir = f\"{datasetOriginalDir}/train\"\n",
    "datasetOriginalTestDir = f\"{datasetOriginalDir}/test\"\n",
    "\n",
    "processedDataDir = \"preprocessed_data\"\n",
    "preTrainDir = f\"{processedDataDir}/{size}/train\"\n",
    "preTestDir = f\"{processedDataDir}/{size}/test\"\n",
    "\n",
    "user2intPath = f\"{preTrainDir}/user2int.tsv\"\n",
    "word2intPath = f\"{preTrainDir}/word2int.tsv\"\n",
    "category2intPath = f\"{preTrainDir}/category2int.tsv\"\n",
    "\n",
    "wordEmbeddingDir = \"word_embeddings\"\n",
    "wordEmbeddingPath = f\"{wordEmbeddingDir}/glove.840B.300d.txt\"\n",
    "\n",
    "os.makedirs(datasetTrainDir, exist_ok=True)\n",
    "os.makedirs(datasetTestDir, exist_ok=True)\n",
    "os.makedirs(datasetOriginalTrainDir, exist_ok=True)\n",
    "os.makedirs(datasetOriginalTestDir, exist_ok=True)\n",
    "os.makedirs(preTrainDir, exist_ok=True)\n",
    "os.makedirs(preTestDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    dataset_dir: str\n",
    "    dataset_original_dir: str\n",
    "    train_out_dir: str\n",
    "    test_out_dir: str\n",
    "    user2int_path: str\n",
    "    split_test_size: float\n",
    "    n_negative: int\n",
    "\n",
    "args = Args(\n",
    "    dataset_dir = datasetDir,\n",
    "    dataset_original_dir = datasetOriginalDir,\n",
    "    train_out_dir = preTrainDir,\n",
    "    test_out_dir = preTestDir,\n",
    "    user2int_path = f\"{preTrainDir}/user2int.tsv\",\n",
    "    split_test_size = 0.1,\n",
    "    n_negative = 4\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "※중요: train_data_number는 2 이상으로 설정해야 합니다.\n",
    "나중에 데이터 전처리 과정에서 train_test_split()을 통해 train 데이터셋을 train/val로 쪼개기 때문입니다.\n",
    "\"\"\"\n",
    "train_data_number = 3000\n",
    "test_data_number = 2000\n",
    "random.seed(1234)\n",
    "\n",
    "def pick_random_integers(min: int, max: int, num: int):\n",
    "    if num > (max - min + 1):\n",
    "        raise ValueError(\"샘플 개수는 데이터의 총 개수보다 적거나 같아야 합니다.\")\n",
    "    elif num < 1:\n",
    "        raise ValueError(\"num은 0보다 커야합니다.\")\n",
    "    return random.sample(range(min, max + 1), num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. manual Train/Test behaviors.tsv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 98310.14it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 117670.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_behaviors_dataset(\n",
    "        original_path: str,\n",
    "        out_path: str,\n",
    "        data_number: int\n",
    "    ):\n",
    "    # 원본 데이터셋을 불러옵니다.\n",
    "    with open(original_path, 'r') as original_behavior_file:\n",
    "        original_behaviors = original_behavior_file.readlines()\n",
    "\n",
    "    # 원본 데이터셋에서 뽑아올 샘플의 인덱스를 랜덤으로 생성합니다.\n",
    "    indexes = pick_random_integers(0, len(original_behaviors) - 1, data_number)\n",
    "\n",
    "    # 새로운 behaviors.tsv 파일을 생성합니다.\n",
    "    with open(out_path, 'w', newline='') as behavior_out_file:\n",
    "        behaviors_writer = csv.writer(behavior_out_file, delimiter='\\t')\n",
    "        for index in tqdm(indexes):\n",
    "            behavior_data = original_behaviors[index]\n",
    "            behaviors_writer.writerow(behavior_data.strip().split('\\t'))\n",
    "\n",
    "generate_behaviors_dataset(\n",
    "    path.join(args.dataset_original_dir, \"train\", \"behaviors.tsv\"),\n",
    "    path.join(args.dataset_dir, \"train\", \"behaviors.tsv\"),\n",
    "    train_data_number\n",
    ")\n",
    "generate_behaviors_dataset(\n",
    "    path.join(args.dataset_original_dir, \"test\", \"behaviors.tsv\"),\n",
    "    path.join(args.dataset_dir, \"test\", \"behaviors.tsv\"),\n",
    "    test_data_number\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. manual Train/Test news.tsv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 93750.51it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 105266.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_news_dataset(\n",
    "        behaviors_path: str,\n",
    "        original_news_path: str,\n",
    "        out_path: str\n",
    "    ):\n",
    "    # 생성한 behaviors.tsv 데이터셋을 불러옵니다.\n",
    "    with open(behaviors_path, 'r') as original_behavior_file:\n",
    "        behaviors = original_behavior_file.readlines()\n",
    "\n",
    "    # 원본 news.tsv 데이터셋을 불러옵니다.\n",
    "    news_columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entitles\", \"abstract_entities\"]\n",
    "    original_news_df = pd.read_csv(original_news_path, sep='\\t', header=None, names=news_columns, encoding='utf-8', index_col=\"news_id\")\n",
    "\n",
    "    # manual/train/behaviors.tsv로 선별한 모든 샘플에 포함된 뉴스 목록을 news.tsv에 저장하기 위한 전처리 과정을 시작합니다.\n",
    "    news_collection = set()\n",
    "    for behavior_data in tqdm(behaviors):\n",
    "        imp_id, user_id, time, history, impressions = behavior_data.strip().split('\\t')\n",
    "        # NewsID가 들어 있는 history, impressions에서 ID만 빼옵니다.\n",
    "        history = history.split(' ')\n",
    "        impressions = [s.split('-')[0] for s in impressions.split(' ')]\n",
    "        # 중복을 제거합니다.\n",
    "        if history[0] != '':\n",
    "            news_collection.update(history)\n",
    "        if impressions[0] != '':\n",
    "            news_collection.update(impressions)\n",
    "\n",
    "    # 중복 없이 뽑아낸 모든 뉴스ID의 데이터를 선별합니다.\n",
    "    news_df = original_news_df.loc[list(news_collection)]\n",
    "\n",
    "    # 선별한 데이터를 news.tsv에 저장합니다.\n",
    "    news_df.to_csv(out_path, sep='\\t', header=None, encoding='utf-8')\n",
    "    return news_df\n",
    "\n",
    "train_news_df = generate_news_dataset(\n",
    "    path.join(args.dataset_dir, \"train\", \"behaviors.tsv\"),\n",
    "    path.join(args.dataset_original_dir, \"train\", \"news.tsv\"),\n",
    "    path.join(args.dataset_dir, \"train\", \"news.tsv\")\n",
    ")\n",
    "test_news_df = generate_news_dataset(\n",
    "    path.join(args.dataset_dir, \"test\", \"behaviors.tsv\"),\n",
    "    path.join(args.dataset_original_dir, \"test\", \"news.tsv\"),\n",
    "    path.join(args.dataset_dir, \"test\", \"news.tsv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17890, 7)\n",
      "(12790, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train_news_df.shape)\n",
    "print(test_news_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
