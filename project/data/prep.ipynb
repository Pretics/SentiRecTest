{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순서 파악파기\n",
    "\n",
    "python parse_behavior.py --in-file \"$datasetTrainPath/behaviors.tsv\" --out-dir \"$preTrainPath\" --mode train\n",
    "\n",
    "python parse_behavior.py --in-file \"$datasetTestPath/behaviors.tsv\" --out-dir \"$preTestPath\" --mode test --user2int \"$preTrainPath/user2int.tsv\"\n",
    "\n",
    "python parse_news.py --in-file \"$datasetTrainPath/news.tsv\" --out-dir \"$preTrainPath\" --mode train --word-embeddings \"$wordEmbeddingPath/glove.840B.300d.txt\"\n",
    "\n",
    "python parse_news.py --in-file \"$datasetTestPath/news.tsv\" --out-dir \"$preTestPath\" --mode test --word-embeddings \"$wordEmbeddingPath/glove.840B.300d.txt\" --embedding-weights \"$preTrainPath/embedding_weights.csv\" --word2int \"$preTrainPath/word2int.tsv\" --category2int \"$preTrainPath/category2int.tsv\"\n",
    "\n",
    "parse_behavior.py 먼저 진행\n",
    "\n",
    "parse_news는 glove 임베딩 파일 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import parse_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = \"demo\"\n",
    "\n",
    "datasetPath = f\"MIND/{size}\"\n",
    "datasetTrainPath = f\"{datasetPath}/train\"\n",
    "datasetTestPath = f\"{datasetPath}/test\"\n",
    "trainBehaviorsPath = f\"{datasetTrainPath}/behaviors.tsv\"\n",
    "testBehaviorsPath = f\"{datasetTestPath}/behaviors.tsv\"\n",
    "wordEmbeddingPath = \"word_embeddings\"\n",
    "\n",
    "processedDataPath = \"processed\"\n",
    "preTrainPath = f\"{processedDataPath}/{size}/train\"\n",
    "preTestPath = f\"{processedDataPath}/{size}/test\"\n",
    "\n",
    "user2int = f\"{preTrainPath}/user2int.tsv\"\n",
    "\n",
    "word_embeddings = f\"{wordEmbeddingPath}/glove.840B.300d.txt\"\n",
    "word2int = f\"{preTrainPath}/word2int.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. python parse_behavior.py --in-file \"$datasetTrainPath/behaviors.tsv\" --out-dir \"$preTrainPath\" --mode train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train behaviors.tsv 전처리\n",
    "\n",
    "#parse_behavior.py용 args\n",
    "args = argparse.Namespace(\n",
    "    in_file = trainBehaviorsPath,\n",
    "    split = 0.1,\n",
    "    out_dir = preTrainPath\n",
    ")\n",
    "\n",
    "with open(args.in_file, 'r') as trainBehaviors:\n",
    "    behavior = trainBehaviors.readlines()\n",
    "    if (args.split == 0):\n",
    "        parse_behavior.generate_training_data(behavior, args.out_dir)\n",
    "    else:\n",
    "        train_behavior, val_behavior = train_test_split(behavior,test_size=args.split, random_state=1234)\n",
    "        user2int = parse_behavior.generate_training_data(train_behavior, args.out_dir)\n",
    "        parse_behavior.generate_eval_data(val_behavior, args.out_dir, \"val_behavior.tsv\", user2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. python parse_behavior.py --in-file \"$datasetTestPath/behaviors.tsv\" --out-dir \"$preTestPath\" --mode test --user2int \"$preTrainPath/user2int.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test behaviors.tsv 전처리\n",
    "\n",
    "#parse_behavior.py용 args\n",
    "args = argparse.Namespace(\n",
    "    in_file = testBehaviorsPath,\n",
    "    user2int = f\"{preTrainPath}/user2int.tsv\",\n",
    "    out_dir = preTrainPath\n",
    ")\n",
    "\n",
    "user2int = parse_behavior.load_idx_map_as_dict(args.user2int)\n",
    "with open(args.in_file, 'r') as in_file:\n",
    "    behavior = in_file.readlines()\n",
    "    parse_behavior.generate_eval_data(behavior, args.out_dir, \"test_behavior.tsv\", user2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. python parse_news.py --in-file \"$datasetTrainPath/news.tsv\" --out-dir \"$preTrainPath\" --mode train --word-embeddings \"$wordEmbeddingPath/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from transformers import pipeline\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import parse_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train behaviors.tsv 전처리\n",
    "\n",
    "#parse_behavior.py용 args\n",
    "args = argparse.Namespace(\n",
    "    in_file = trainBehaviorsPath,\n",
    "    split = 0.1,\n",
    "    out_dir = preTrainPath\n",
    ")\n",
    "\n",
    "# prep embedings/vocab\n",
    "embeddings = process_word_embeddings(args.word_embeddings)\n",
    "\n",
    "    \n",
    "# parse news \n",
    "with open(args.in_file, 'r', encoding='utf-8') as in_file:\n",
    "    with open(path.join(args.out_dir, 'parsed_news.tsv'), 'w', newline='') as news_file:  \n",
    "        news_writer = csv.writer(news_file, delimiter='\\t')\n",
    "        print(\"preparing/processing news content\")\n",
    "        news_collection = in_file.readlines()\n",
    "        news2int = {}\n",
    "        # sentiment analyzer\n",
    "        dsb_sentiment_classifier = pipeline('sentiment-analysis')\n",
    "        vader_sentiment_classifier = SentimentIntensityAnalyzer()\n",
    "        # max title/abstract length\n",
    "        max_title_length = int(args.max_title)\n",
    "        max_abstract_length = int(args.max_abstract)\n",
    "        if args.mode == \"train\": \n",
    "            category2int = {}\n",
    "            word2int = {}\n",
    "            embedding_weights = []\n",
    "        else:\n",
    "            category2int = load_idx_map_as_dict(args.category2int)\n",
    "            word2int = load_idx_map_as_dict(args.word2int)\n",
    "            embedding_weights = load_embedding_weights(args.embedding_weights)\n",
    "            \n",
    "        # iterate over news\n",
    "        for news in tqdm(news_collection):\n",
    "            newsid, category, subcategory, title, abstract, _, _, _ = news.strip().split(\"\\t\")\n",
    "            if newsid not in news2int:\n",
    "                news2int[newsid] = len(news2int) + 1\n",
    "            else:\n",
    "                continue\n",
    "            # category to int\n",
    "            if category not in category2int:\n",
    "                if(args.mode == \"train\"):\n",
    "                    category2int[category] = len(category2int) + 1\n",
    "                    category_id = category2int[category]\n",
    "                else:\n",
    "                    category_id = 0\n",
    "            else: \n",
    "                category_id = category2int[category]\n",
    "            if subcategory not in category2int:\n",
    "                if(args.mode == \"train\"):\n",
    "                    category2int[subcategory] = len(category2int) + 1\n",
    "                    subcategory_id = category2int[subcategory]\n",
    "                else:\n",
    "                    subcategory_id = 0\n",
    "            else: \n",
    "                subcategory_id = category2int[subcategory]\n",
    "            # parse/prep title --> to token ids\n",
    "            # crop at max-title or pad to max-title\n",
    "            title_tokens = word_tokenize(title.strip().lower())\n",
    "            title_word_idxs = []\n",
    "            for token in title_tokens:\n",
    "                if token not in embeddings:\n",
    "                    continue\n",
    "                if token not in word2int:\n",
    "                    word2int[token] = str(len(word2int) + 1)\n",
    "                    embedding_weights.append(embeddings[token])\n",
    "                title_word_idxs.append(word2int[token])\n",
    "            # title_word_idxs = [word2int[token] for token in title_tokens if token in word2int]\n",
    "            if len(title_word_idxs) > max_title_length:\n",
    "                title_word_idxs = title_word_idxs[:max_title_length]\n",
    "            else:\n",
    "                title_word_idxs = title_word_idxs + [\"0\"]*(max_title_length-len(title_word_idxs))\n",
    "            title_word_idxs_str = \" \".join(title_word_idxs)\n",
    "            # parse/prep abstract --> to token ids\n",
    "            # crop at max-abstract or pad to max-abstract\n",
    "            abstract_tokens = word_tokenize(abstract.strip().lower())\n",
    "            abstract_word_idxs = []\n",
    "            for token in abstract_tokens:\n",
    "                if token not in embeddings:\n",
    "                    continue\n",
    "                if token not in word2int:\n",
    "                    word2int[token] = str(len(word2int) + 1)\n",
    "                    embedding_weights.append(embeddings[token])\n",
    "                abstract_word_idxs.append(word2int[token])\n",
    "            if len(abstract_word_idxs) > max_abstract_length:\n",
    "                abstract_word_idxs = abstract_word_idxs[:max_abstract_length]\n",
    "            else:\n",
    "                abstract_word_idxs = abstract_word_idxs + [\"0\"]*(max_abstract_length-len(abstract_word_idxs))\n",
    "            abstract_word_idxs_str = \" \".join(abstract_word_idxs)\n",
    "            # calc sentiments scores\n",
    "            # vader\n",
    "            vs = vader_sentiment_classifier.polarity_scores(title.strip())\n",
    "            vader_sentiment = vs['compound']\n",
    "            # bert\n",
    "            dsbs_label, dsbs_score = dsb_sentiment_classifier(title.strip())[0].values()\n",
    "            if(dsbs_label == \"POSITIVE\"):\n",
    "                bert_sentiment = (1-dsbs_score)*(-1) + dsbs_score\n",
    "            else:\n",
    "                bert_sentiment = (dsbs_score)*(-1) + (1-dsbs_score)\n",
    "            # prepare output\n",
    "            news_writer.writerow([\n",
    "                newsid,\n",
    "                category_id,\n",
    "                subcategory_id,\n",
    "                title_word_idxs_str,\n",
    "                abstract_word_idxs_str,\n",
    "                vader_sentiment,\n",
    "                bert_sentiment\n",
    "            ])\n",
    "        if args.mode == \"train\":\n",
    "            with open(path.join(args.out_dir, 'category2int.tsv'), 'w', encoding='utf-8', newline='') as file:  \n",
    "                cat_writer = csv.writer(file, delimiter='\\t')\n",
    "                for key, value in category2int.items():\n",
    "                    cat_writer.writerow([key, value])\n",
    "        with open(path.join(args.out_dir, 'word2int.tsv'), 'w', encoding='utf-8', newline='') as file:\n",
    "            word_writer = csv.writer(file, delimiter='\\t')\n",
    "            for key, value in word2int.items():\n",
    "                word_writer.writerow([key, value])\n",
    "        with open(path.join(args.out_dir, 'embedding_weights.csv'), 'w', encoding='utf-8', newline='') as file:\n",
    "            # 첫 줄에 index=0(빈칸) 패딩용 0 0 0 ... 0 가중치 추가\n",
    "            padding_weights = \" \".join([\"0\"] * 300) + \"\\n\"\n",
    "            file.write(padding_weights)\n",
    "            for weights in embedding_weights:\n",
    "                file.write(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
