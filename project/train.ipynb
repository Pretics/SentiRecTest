{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook에서 import 해서 쓰는 .py 모듈의 코드가 변경될 시, 변동 사항을 자동으로 반영해주는 기능 ON\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "from dotmap import DotMap\n",
    "from os import path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from models.lstur import LSTUR\n",
    "from models.nrms import NRMS\n",
    "from models.naml import NAML\n",
    "from models.naml_simple import NAML_Simple\n",
    "from models.sentirec import SENTIREC\n",
    "from models.robust_sentirec import ROBUST_SENTIREC\n",
    "from data.dataset import BaseDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1234\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    config = \"config/model/nrms/exp1.yaml\",\n",
    "    resume = None\n",
    ")\n",
    "\n",
    "with open(args.config, 'r') as ymlfile:\n",
    "    config = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    config = DotMap(config)\n",
    "\n",
    "assert(config.name in [\"lstur\", \"nrms\", \"naml\", \"naml_simple\", \"sentirec\", \"robust_sentirec\"])\n",
    "\n",
    "pl.seed_everything(1234)\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    **config.logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    **config.checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26740/26740 [00:01<00:00, 14749.41it/s]\n",
      "100%|██████████| 28994/28994 [00:14<00:00, 2066.49it/s]\n",
      "100%|██████████| 26740/26740 [00:01<00:00, 13958.46it/s]\n",
      "100%|██████████| 2204/2204 [00:02<00:00, 1075.78it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_path = f\"{config.preprocess_data_path}/{config.dataset_size}/\"\n",
    "\n",
    "train_dataset = BaseDataset(\n",
    "    path.join(preprocess_path+config.train_behavior),\n",
    "    path.join(preprocess_path+config.train_news), \n",
    "    config)\n",
    "val_dataset = BaseDataset(\n",
    "    path.join(preprocess_path+config.val_behavior),\n",
    "    path.join(preprocess_path+config.train_news), \n",
    "    config) \n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    **config.train_dataloader)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    **config.val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42562/42562 [00:03<00:00, 13072.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# load embedding pre-trained embedding weights\n",
    "embedding_weights=[]\n",
    "with open(path.join(preprocess_path+config.embedding_weights), 'r') as file: \n",
    "    lines = file.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        weights = [float(w) for w in line.split(\" \")]\n",
    "        embedding_weights.append(weights)\n",
    "pretrained_word_embedding = torch.from_numpy(\n",
    "    np.array(embedding_weights, dtype=np.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.name == \"lstur\":\n",
    "        model = LSTUR(config, pretrained_word_embedding)\n",
    "elif config.name == \"nrms\":\n",
    "    model = NRMS(config, pretrained_word_embedding)\n",
    "elif config.name == \"naml\":\n",
    "    model = NAML(config, pretrained_word_embedding)\n",
    "elif config.name == \"naml_simple\":\n",
    "    model = NAML_Simple(config, pretrained_word_embedding)\n",
    "elif config.name == \"sentirec\":\n",
    "    model = SENTIREC(config, pretrained_word_embedding)\n",
    "elif config.name == \"robust_sentirec\":\n",
    "    model = ROBUST_SENTIREC(config, pretrained_word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    **config.early_stop\n",
    ")\n",
    "if args.resume is not None:\n",
    "    model = model.load_from_checkpoint(\n",
    "        args.resume, \n",
    "        config=config, \n",
    "        pretrained_word_embedding=pretrained_word_embedding)\n",
    "    trainer = Trainer(\n",
    "        **config.trainer,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=logger,\n",
    "        #strategy=DDPStrategy(process_group_backend=\"gloo\"),\n",
    "        #plugins=DDPPlugin(find_unused_parameters=config.find_unused_parameters), \n",
    "        resume_from_checkpoint=args.resume\n",
    "    )\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        **config.trainer,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=logger,\n",
    "        #strategy=DDPStrategy(process_group_backend=\"gloo\")\n",
    "        #plugins=DDPPlugin(find_unused_parameters=config.find_unused_parameters)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                                   | Type             | Params | Mode \n",
      "-------------------------------------------------------------------------------------\n",
      "0  | news_encoder                           | TimeDistributed  | 13.2 M | train\n",
      "1  | user_encoder                           | UserEncoder      | 421 K  | train\n",
      "2  | val_performance_metrics                | MetricCollection | 0      | train\n",
      "3  | val_sentiment_diversity_metrics_vader  | MetricCollection | 0      | train\n",
      "4  | val_sentiment_diversity_metrics_bert   | MetricCollection | 0      | train\n",
      "5  | test_performance_metrics               | MetricCollection | 0      | train\n",
      "6  | test_sentiment_diversity_metrics_vader | MetricCollection | 0      | train\n",
      "7  | test_sentiment_diversity_metrics_bert  | MetricCollection | 0      | train\n",
      "8  | test_topic_diversity_metrics           | MetricCollection | 0      | train\n",
      "9  | test_ils_senti_metrics_vader           | MetricCollection | 0      | train\n",
      "10 | test_ils_senti_metrics_bert            | MetricCollection | 0      | train\n",
      "11 | test_ils_topic_metrics                 | MetricCollection | 0      | train\n",
      "-------------------------------------------------------------------------------------\n",
      "13.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.6 M    Total params\n",
      "54.447    Total estimated model params size (MB)\n",
      "51        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  35%|███▌      | 159/453 [00:19<00:35,  8.25it/s, v_num=exp1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:282\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    281\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    763\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03mTake in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m    >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    169\u001b[0m clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m--> 171\u001b[0m     {\n\u001b[0;32m    172\u001b[0m         key: collate(\n\u001b[0;32m    173\u001b[0m             [d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map\n\u001b[0;32m    174\u001b[0m         )\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m     }\n\u001b[0;32m    177\u001b[0m )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    169\u001b[0m clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    171\u001b[0m     {\n\u001b[1;32m--> 172\u001b[0m         key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m     }\n\u001b[0;32m    177\u001b[0m )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=model, \n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders=val_loader\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
