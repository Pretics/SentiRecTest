{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "behaviors.tsv 안의 단일 impression 데이터가 갖는 구조를 잠시 살펴보겠습니다. <br/>\n",
    "(https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md) <br/>\n",
    " <br/>\n",
    " Impression ID: `int` <br/>\n",
    " User ID: `str` <br/>\n",
    " Time: `str` <br/>\n",
    " History: `[News_ID, News_ID, News_ID ...]` <br/>\n",
    " Impressions: `['News_ID-0', 'News_ID-1', ...]` <br/>\n",
    " <br/>\n",
    "그리고 하나의 뉴스가 갖는 데이터 구조는 다음과 같습니다. <br/>\n",
    " <br/>\n",
    "News ID: `str` <br/>\n",
    "Category: `str` <br/>\n",
    "SubCategory: `str` <br/>\n",
    "Title: `str` <br/>\n",
    "Abstract: `str` <br/>\n",
    "(그 외 URL, Title Entities, Abstract Entities - 사용하지 않음) <br/>\n",
    " <br/>\n",
    "이 프로젝트에서는 Title, Abstract의 모든 단어(token)를 인덱스로 치환합니다. <br/>\n",
    "(어떤 식으로 치환하는지는 word2int를 참고해주세요.) <br/>\n",
    "그리고 모든 단어는 사전 학습된 GloVe의 embedding lookup table에서 단어에 해당하는 임베딩 벡터를 가져와서 embedding_weights에 저장합니다. <br/>\n",
    "(GloVe word embedding 파일(약 8GB)에는 약 800만개 가량의 단어와, 해당 단어를 300차원 임베딩 벡터로 변환한 데이터가 저장되어 있습니다.) <br/>\n",
    "그리고 word2int로 저장되는 인덱스는 embedding_weights의 맨 첫번째 줄의 padding을 제외하고 모두 1대1로 대응됩니다. <br/>\n",
    "즉 embedding_weights를 불러왔을 때 5번 인덱스의 벡터는 word2int의 5번 인덱스에 해당하는 단어의 임베딩 벡터가 됩니다. <br/>\n",
    " <br/>\n",
    "word2int에는 0번 인덱스가 없습니다. 왜냐하면 0번 인덱스는 모든 문장의 token 길이를 동일하게 맞추는 과정에서 생기는 공백을 매꾸는 데 쓰이기 때문입니다. (padding) <br/>\n",
    " <br/>\n",
    "(이로 인해 생기는 문제가 하나 있는데, embedding_weights 맨 첫번째 줄에 padding용 300차원 영벡터를 추가적으로 생성하지 않은 초기 버전 프로젝트는 <br/>\n",
    "word2int 맨 마지막 인덱스에 해당하는 단어가 포함된 뉴스의 데이터를 불러올 때 out of bounds 에러가 떴습니다.) <br/>\n",
    " <br/>\n",
    "(`자세한 설명`: word2int에 저장된 모든 단어의 수를 max_word_num이라고 할 때, <br/>\n",
    "word2int에 매핑된 단어들의 index 범위: 1 ~ max_word_num <br/>\n",
    "embedding_weights에 저장된 임베딩 벡터의 수: max_word_num <br/>\n",
    "embedding_weights를 불러왔을 때 생성되는 데이터의 index 범위: 0 ~ (max_word_num-1) <br/>\n",
    "따라서 word2int의 index를 그대로 사용할 경우, <br/>\n",
    "데이터에 마지막 인덱스에 해당하는 단어의 임베딩 벡터를 불러오려 하면 오류가 발생합니다. <br/>\n",
    "그런데 테스트에 주로 사용하는 SENTIREC, NRMS 모델은 Abstract 데이터를 사용하지 않기 때문에, 마지막 인덱스 단어가 Abstract에만 있을 경우 해당 오류가 뜨지 않습니다. <br/>\n",
    "즉 어떤 데이터셋을 사용하였느냐, 전처리가 어떤 순서로 진행되었느냐에 따라 발생 유무, 타이밍이 오락가락 한다는 것입니다. <br/>\n",
    "또한 test의 embedding_weights가 모든 데이터의 단어를 포함하며, test 데이터셋을 마지막으로 처리하는 특성상 train 과정에서는 오류가 발생하지 않습니다. <br/>\n",
    "그리고 가장 큰 문제는 임베딩 벡터가 한 줄씩 밀려서, 불러온 값이 아예 다른 단어의 임베딩 벡터가 된다는 점입니다. <br/>\n",
    "물론 지금은 전처리 과정에서 embedding_weights 파일 첫 줄에 padding 벡터를 추가해주기 때문에 이러한 문제가 발생하지 않습니다.) <br/>\n",
    " <br/>\n",
    "이러한 인덱스 치환의 결과는 parsed_news.tsv 파일에서 확인할 수 있습니다. <br/>\n",
    "해당 파일에 저장하는 데이터는 다음과 같이 구성됩니다. <br/>\n",
    " <br/>\n",
    "`News ID, Category, SubCategory, Title, Abstract, VADER Sentiment Score, BERT Sentiment Score`  <br/>\n",
    " <br/>\n",
    "여기서 Category, SubCategory는 category2int.tsv에 매핑된 인덱스에 맞게 치환되고 <br/>\n",
    "Title, Abstract는 전처리 과정에서 설정한 max_title, max_abstract 값에 맞게 token 길이가 고정됩니다. <br/>\n",
    "즉 너무 길면 잘리고, 너무 짧으면 padding(index = 0)으로 채워집니다. <br/>\n",
    " <br/>\n",
    "behaviors.tsv의 경우, train 데이터셋은 train_behavior.tsv와 val_behavior.tsv, test 데이터셋은 test_behavior.tsv로 변환됩니다. <br/>\n",
    "구성은 모두 동일하나, 모델의 학습/테스트 과정에서 사용처가 다릅니다. <br/>\n",
    "_behavior.tsv 파일의 데이터 구성은 다음과 같습니다. <br/>\n",
    " <br/>\n",
    "`User ID, History, Impressions, Labels` <br/>\n",
    " <br/>\n",
    "공통: User ID는 user2int.tsv에 매핑된 인덱스로 변환되며, Impressions의 클릭 유무를 나타내는 모든 Label이 따로 떼어집니다. <br/>\n",
    "train_behavior.tsv와 val_behavior.tsv의 경우: <br/>\n",
    " negative sampling이 적용되어 positive sample(label = 1) 하나당 n_negative로 설정한 개수의 negative sample(label = 0)만 사용합니다. <br/>\n",
    " 또한 positive sample이 여러개일 경우, 한 Impression 데이터를 여러 줄로 쪼개어 사용합니다. <br/>\n",
    "반면 test_behavior.tsv는 negative sampling을 하지도 않고, 한 Impression 데이터를 여러 줄로 쪼개지도 않습니다. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook에서 import 해서 쓰는 모듈의 코드가 변경될 시, 변동 사항을 자동으로 반영해주는 기능\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\projects\\python\\newsrecommend\\SentiRecTest\\project\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "module import를 진행할 시, sys.path에 등록된 경로에서 해당 모듈 파일을 찾습니다.\n",
    "그런데 프로젝트 폴더가 기본적으로 등록되어있지 않아서\n",
    "project/data 같은 경로의 모듈을 사용하기 위해 둘 중 한가지 방법을 써야 합니다.\n",
    "직접 경로를 추가 => sys.path.append(...)\n",
    "import할 때 상대 경로 사용 => from ...data.preprocess import\n",
    "그런데 상대 경로가 이래저래 요상한 점이 많아서\n",
    "절대 경로를 등록해서 사용하기로 했습니다.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "PROJECT_DIR = path.abspath(path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(PROJECT_DIR)\n",
    "print(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\newsrec\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data.preprocess import prep_behavior\n",
    "from data.preprocess import prep_news\n",
    "from data.preprocess.prep_behavior import PrepBehaviorArgs\n",
    "from data.preprocess.prep_news import PrepNewsArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "경로가 폴더를 나타낼 경우 Dir, 파일일 경우 Path로 명명\n",
    "\n",
    "size: 전처리를 진행할 데이터셋의 크기 (demo, small, large 등)\n",
    "\"\"\"\n",
    "\n",
    "size = \"demo\"\n",
    "\n",
    "DATA_DIR = path.join(PROJECT_DIR, \"data\")\n",
    "datasetDir = path.join(DATA_DIR, \"MIND\", size)\n",
    "trainBehaviorsPath = path.join(datasetDir, \"train\", \"behaviors.tsv\")\n",
    "testBehaviorsPath = path.join(datasetDir, \"test\", \"behaviors.tsv\")\n",
    "trainNewsPath = path.join(datasetDir, \"train\", \"news.tsv\")\n",
    "testNewsPath = path.join(datasetDir, \"test\", \"news.tsv\")\n",
    "\n",
    "processedDataDir = path.join(DATA_DIR, \"preprocessed_data\")\n",
    "preTrainDir = path.join(processedDataDir, size, \"train\")\n",
    "preTestDir = path.join(processedDataDir, size, \"test\")\n",
    "\n",
    "wordEmbeddingDir = path.join(DATA_DIR, \"word_embeddings\")\n",
    "wordEmbeddingPath = path.join(wordEmbeddingDir, \"glove.840B.300d.txt\")\n",
    "wordEmbeddingNpyPath = path.join(wordEmbeddingDir, \"glove.840B.300d.npy\")\n",
    "wordEmbeddingTokensPath = path.join(wordEmbeddingDir, \"glove.840B.300d.tokens.tsv\")\n",
    "\n",
    "os.makedirs(preTrainDir, exist_ok=True)\n",
    "os.makedirs(preTestDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train/Test 데이터셋 behaviors.tsv 전처리\n",
    "\n",
    "### 생성되는 파일\n",
    "##### Train\n",
    "1. train_behavior.tsv\n",
    "2. user2int.tsv\n",
    "3. val_behavior.tsv\n",
    "##### Test\n",
    "1. test_behavior.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = PrepBehaviorArgs(\n",
    "    train_behavior_path = trainBehaviorsPath,\n",
    "    test_behavior_path = testBehaviorsPath,\n",
    "    train_out_dir = preTrainDir,\n",
    "    test_out_dir = preTestDir,\n",
    "    user2int_path = f\"{preTrainDir}/user2int.tsv\",\n",
    "    split_test_size = 0.2,\n",
    "    n_negative = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17627/17627 [00:00<00:00, 46320.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing eval data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [00:00<00:00, 47900.80it/s]\n",
      "100%|██████████| 4729/4729 [00:00<00:00, 2367776.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing eval data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7538/7538 [00:00<00:00, 63072.87it/s]\n"
     ]
    }
   ],
   "source": [
    "prep_behavior.prep_behavior(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test 데이터셋 news.tsv 전처리\n",
    "\n",
    "### 생성되는 파일\n",
    "##### Train\n",
    "1. parsed_news.tsv\n",
    "2. category2int.tsv\n",
    "3. embedding_weights.csv\n",
    "4. word2int.tsv\n",
    "##### Test\n",
    "1. parsed_news.tsv\n",
    "2. embedding_weights.csv\n",
    "3. word2int.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_news_combined.py용 args\n",
    "args = PrepNewsArgs(\n",
    "    train_news_path = trainNewsPath,\n",
    "    test_news_path = testNewsPath,\n",
    "    train_out_dir = preTrainDir,\n",
    "    test_out_dir = preTestDir,\n",
    "    word_embedding_path = wordEmbeddingPath,\n",
    "    word_embedding_npy_path = wordEmbeddingNpyPath,\n",
    "    word_embedding_tokens_path = wordEmbeddingTokensPath,\n",
    "    max_title = 20,\n",
    "    max_abstract = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load word-embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196017/2196017 [00:01<00:00, 1404005.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# prep embedings/vocab\n",
    "embeddings = prep_news.load_word_embeddings_by_npy(args.word_embedding_npy_path, args.word_embedding_tokens_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing/processing train news content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "  0%|          | 1/26740 [00:00<1:05:21,  6.82it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 26740/26740 [02:07<00:00, 210.28it/s]\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing/processing test news content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "100%|██████████| 18723/18723 [01:27<00:00, 214.15it/s]\n"
     ]
    }
   ],
   "source": [
    "prep_news.prep_news(args, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타 테스트 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "2196017\n",
      "<class 'numpy.ndarray'>\n",
      "[ 0.25233    0.10176   -0.67485    0.21117    0.43492    0.16542\n",
      "  0.48261   -0.81222    0.041321   0.78502   -0.077857  -0.66324\n",
      "  0.1464    -0.29289   -0.25488    0.019293  -0.20265    0.98232\n",
      "  0.028312  -0.081276  -0.1214     0.13126   -0.17648    0.13556\n",
      " -0.16361   -0.22574    0.055006  -0.20308    0.20718    0.095785\n",
      "  0.22481    0.21537   -0.32982   -0.12241   -0.40031   -0.079381\n",
      " -0.19958   -0.015083  -0.079139  -0.18132    0.20681   -0.36196\n",
      " -0.30744   -0.24422   -0.23113    0.09798    0.1463    -0.062738\n",
      "  0.42934   -0.078038  -0.19627    0.65093   -0.22807   -0.30308\n",
      " -0.12483   -0.17568   -0.14651    0.15361   -0.29518    0.15099\n",
      " -0.51726   -0.033564  -0.23109   -0.7833     0.018029  -0.15719\n",
      "  0.02293    0.49639    0.029225   0.05669    0.14616   -0.19195\n",
      "  0.16244    0.23898    0.36431    0.45263    0.2456     0.23803\n",
      "  0.31399    0.3487    -0.035791   0.56108   -0.25345    0.051964\n",
      " -0.10618   -0.30962    1.0585    -0.42025    0.18216   -0.11256\n",
      "  0.40576    0.11784   -0.19705   -0.075292   0.080723  -0.02782\n",
      " -0.15617   -0.44681   -0.15165    0.1692     0.098255  -0.031894\n",
      "  0.087143   0.26082    0.002706   0.1319     0.34439   -0.37894\n",
      " -0.4114     0.081571  -0.11674   -0.43711    0.011144   0.099353\n",
      "  0.26612    0.40025    0.18895   -0.18438   -0.30355   -0.2725\n",
      "  0.22468   -0.40614    0.15618   -0.16043    0.47147    0.0080203\n",
      "  0.56858    0.21934   -0.11181    0.79925    0.10714   -0.50146\n",
      "  0.063593   0.069465   0.15292   -0.2747    -0.20989    0.20737\n",
      " -0.10681    0.40651   -2.6438    -0.31139   -0.32157   -0.26458\n",
      " -0.35625    0.070013  -0.18838    0.48773   -0.26167   -0.020805\n",
      "  0.17819    0.15758   -0.13752    0.056464   0.30766   -0.066136\n",
      "  0.4748    -0.27335    0.09732   -0.20832    0.0039332  0.346\n",
      " -0.08702   -0.54924   -0.18759   -0.17174    0.060324  -0.13521\n",
      "  0.10419    0.30165    0.05798    0.21872   -0.073594  -0.20423\n",
      " -0.25279   -0.10471   -0.32163    0.12525   -0.31281    0.0097207\n",
      " -0.26777   -0.61121   -0.11089   -0.13652    0.035135  -0.4939\n",
      "  0.084857  -0.15494   -0.063509  -0.23935    0.28272    0.10849\n",
      " -0.3365    -0.60764    0.38576   -0.0095438  0.17499   -0.52723\n",
      "  0.62211    0.19544   -0.48977    0.036582  -0.128     -0.016827\n",
      "  0.25647   -0.31698    0.48257   -0.14184    0.11046   -0.3098\n",
      " -0.63141   -0.37268    0.23183   -0.14268   -0.02341    0.022255\n",
      " -0.044662  -0.16404   -0.25848    0.1629     0.024751   0.23348\n",
      "  0.27933    0.38998   -0.058968   0.11355    0.15673    0.18583\n",
      " -0.19814   -0.48123   -0.035084   0.078458  -0.49833    0.10855\n",
      " -0.20133    0.05292   -0.11583   -0.16009    0.16768    0.42362\n",
      " -0.23106    0.082465   0.24296   -0.16786    0.0080409  0.085947\n",
      "  0.38033    0.072981   0.1633     0.24704   -0.11094    0.15115\n",
      " -0.22068   -0.061944  -0.037091  -0.087923  -0.23181    0.15035\n",
      " -0.19093   -0.19113   -0.11894    0.094908  -0.0043347  0.15362\n",
      " -0.41201   -0.3073     0.18375    0.40206   -0.0034793 -0.10917\n",
      " -0.69522    0.10161   -0.079256   0.40329    0.22285   -0.19374\n",
      " -0.13315    0.073231   0.099832   0.11685   -0.21643   -0.1108\n",
      "  0.10341    0.097286   0.11196   -0.3894    -0.0089363  0.28809\n",
      " -0.10792    0.028811   0.32545    0.26052   -0.038941   0.075204\n",
      "  0.46031   -0.06293    0.21661    0.17869   -0.51917    0.33591  ]\n"
     ]
    }
   ],
   "source": [
    "# embeddings 테스트\n",
    "print(type(embeddings))\n",
    "print(len(embeddings))\n",
    "print(type(embeddings[\"hello\"]))\n",
    "print(embeddings[\"hello\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
