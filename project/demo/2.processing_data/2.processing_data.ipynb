{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "behaviors.tsv 안의 단일 impression 데이터가 갖는 구조를 잠시 살펴보겠습니다. <br/>\n",
    "(https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md) <br/>\n",
    " <br/>\n",
    " Impression ID: `int` <br/>\n",
    " User ID: `str` <br/>\n",
    " Time: `str` <br/>\n",
    " History: `[News_ID, News_ID, News_ID ...]` <br/>\n",
    " Impressions: `['News_ID-0', 'News_ID-1', ...]` <br/>\n",
    " <br/>\n",
    "그리고 하나의 뉴스가 갖는 데이터 구조는 다음과 같습니다. <br/>\n",
    " <br/>\n",
    "News ID: `str` <br/>\n",
    "Category: `str` <br/>\n",
    "SubCategory: `str` <br/>\n",
    "Title: `str` <br/>\n",
    "Abstract: `str` <br/>\n",
    "(그 외 URL, Title Entities, Abstract Entities - 사용하지 않음) <br/>\n",
    " <br/>\n",
    "이 프로젝트에서는 Title, Abstract의 모든 단어(token)를 인덱스로 치환합니다. <br/>\n",
    "(어떤 식으로 치환하는지는 word2int를 참고해주세요.) <br/>\n",
    "그리고 모든 단어는 사전 학습된 GloVe의 embedding lookup table에서 단어에 해당하는 임베딩 벡터를 가져와서 embedding_weights에 저장합니다. <br/>\n",
    "(GloVe word embedding 파일(약 8GB)에는 약 800만개 가량의 단어와, 해당 단어를 300차원 임베딩 벡터로 변환한 데이터가 저장되어 있습니다.) <br/>\n",
    "그리고 word2int로 저장되는 인덱스는 embedding_weights의 맨 첫번째 줄의 padding을 제외하고 모두 1대1로 대응됩니다. <br/>\n",
    "즉 embedding_weights를 불러왔을 때 5번 인덱스의 벡터는 word2int의 5번 인덱스에 해당하는 단어의 임베딩 벡터가 됩니다. <br/>\n",
    " <br/>\n",
    "word2int에는 0번 인덱스가 없습니다. 왜냐하면 0번 인덱스는 모든 문장의 token 길이를 동일하게 맞추는 과정에서 생기는 공백을 매꾸는 데 쓰이기 때문입니다. (padding) <br/>\n",
    " <br/>\n",
    "(이로 인해 생기는 문제가 하나 있는데, embedding_weights 맨 첫번째 줄에 padding용 300차원 영벡터를 추가적으로 생성하지 않은 초기 버전 프로젝트는 <br/>\n",
    "word2int 맨 마지막 인덱스에 해당하는 단어가 포함된 뉴스의 데이터를 불러올 때 out of bounds 에러가 떴습니다.) <br/>\n",
    " <br/>\n",
    "(`자세한 설명`: word2int에 저장된 모든 단어의 수를 max_word_num이라고 할 때, <br/>\n",
    "word2int에 매핑된 단어들의 index 범위: 1 ~ max_word_num <br/>\n",
    "embedding_weights에 저장된 임베딩 벡터의 수: max_word_num <br/>\n",
    "embedding_weights를 불러왔을 때 생성되는 데이터의 index 범위: 0 ~ (max_word_num-1) <br/>\n",
    "따라서 word2int의 index를 그대로 사용할 경우, <br/>\n",
    "데이터에 마지막 인덱스에 해당하는 단어의 임베딩 벡터를 불러오려 하면 오류가 발생합니다. <br/>\n",
    "그런데 테스트에 주로 사용하는 SENTIREC, NRMS 모델은 Abstract 데이터를 사용하지 않기 때문에, 마지막 인덱스 단어가 Abstract에만 있을 경우 해당 오류가 뜨지 않습니다. <br/>\n",
    "즉 어떤 데이터셋을 사용하였느냐, 전처리가 어떤 순서로 진행되었느냐에 따라 발생 유무, 타이밍이 오락가락 한다는 것입니다. <br/>\n",
    "또한 test의 embedding_weights가 모든 데이터의 단어를 포함하며, test 데이터셋을 마지막으로 처리하는 특성상 train 과정에서는 오류가 발생하지 않습니다. <br/>\n",
    "그리고 가장 큰 문제는 임베딩 벡터가 한 줄씩 밀려서, 불러온 값이 아예 다른 단어의 임베딩 벡터가 된다는 점입니다. <br/>\n",
    "물론 지금은 전처리 과정에서 embedding_weights 파일 첫 줄에 padding 벡터를 추가해주기 때문에 이러한 문제가 발생하지 않습니다.) <br/>\n",
    " <br/>\n",
    "이러한 인덱스 치환의 결과는 parsed_news.tsv 파일에서 확인할 수 있습니다. <br/>\n",
    "해당 파일에 저장하는 데이터는 다음과 같이 구성됩니다. <br/>\n",
    " <br/>\n",
    "`News ID, Category, SubCategory, Title, Abstract, VADER Sentiment Score, BERT Sentiment Score`  <br/>\n",
    " <br/>\n",
    "여기서 Category, SubCategory는 category2int.tsv에 매핑된 인덱스에 맞게 치환되고 <br/>\n",
    "Title, Abstract는 전처리 과정에서 설정한 max_title, max_abstract 값에 맞게 token 길이가 고정됩니다. <br/>\n",
    "즉 너무 길면 잘리고, 너무 짧으면 padding(index = 0)으로 채워집니다. <br/>\n",
    " <br/>\n",
    "behaviors.tsv의 경우, train 데이터셋은 train_behavior.tsv와 val_behavior.tsv, test 데이터셋은 test_behavior.tsv로 변환됩니다. <br/>\n",
    "구성은 모두 동일하나, 모델의 학습/테스트 과정에서 사용처가 다릅니다. <br/>\n",
    "_behavior.tsv 파일의 데이터 구성은 다음과 같습니다. <br/>\n",
    " <br/>\n",
    "`User ID, History, Impressions, Labels` <br/>\n",
    " <br/>\n",
    "공통: User ID는 user2int.tsv에 매핑된 인덱스로 변환되며, Impressions의 클릭 유무를 나타내는 모든 Label이 따로 떼어집니다. <br/>\n",
    "train_behavior.tsv와 val_behavior.tsv의 경우: <br/>\n",
    " negative sampling이 적용되어 positive sample(label = 1) 하나당 n_negative로 설정한 개수의 negative sample(label = 0)만 사용합니다. <br/>\n",
    " 또한 positive sample이 여러개일 경우, 한 Impression 데이터를 여러 줄로 쪼개어 사용합니다. <br/>\n",
    "반면 test_behavior.tsv는 negative sampling을 하지도 않고, 한 Impression 데이터를 여러 줄로 쪼개지도 않습니다. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook에서 import 해서 쓰는 모듈의 코드가 변경될 시, 변동 사항을 자동으로 반영해주는 기능 켜기\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\projects\\python\\newsrecommend\\SentiRecTest\\project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\newsrec\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "module import를 진행할 시, sys.path에 등록된 경로에서 해당 모듈 파일을 찾습니다.\n",
    "그런데 프로젝트 폴더가 기본적으로 등록되어있지 않아서\n",
    "project/data 같은 경로의 모듈을 사용하기 위해 둘 중 한가지 방법을 써야 합니다.\n",
    "직접 경로를 추가 => sys.path.append(...)\n",
    "import할 때 상대 경로 사용 => from ...data.preprocess import\n",
    "그런데 상대 경로가 이래저래 요상한 점이 많아서\n",
    "절대 경로를 등록해서 사용하기로 했습니다.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "PROJECT_DIR = path.abspath(path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(PROJECT_DIR)\n",
    "print(PROJECT_DIR)\n",
    "\n",
    "from data.preprocess.prep_dataset import PrepDataset, PrepDatasetArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = PrepDatasetArgs(\n",
    "    size = \"demo\",\n",
    "    split_test_size = 0.1,\n",
    "    n_negative = 4,\n",
    "    max_title = 20,\n",
    "    max_abstract = 50\n",
    ")\n",
    "\n",
    "prep_dataset = PrepDataset(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train/Test 데이터셋 behaviors.tsv 전처리\n",
    "\n",
    "### 생성되는 파일\n",
    "##### Train\n",
    "1. train_behavior.tsv\n",
    "2. user2int.tsv\n",
    "3. val_behavior.tsv\n",
    "##### Test\n",
    "1. test_behavior.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test 데이터셋 news.tsv 전처리 (통합됨)\n",
    "\n",
    "### 생성되는 파일\n",
    "##### Train\n",
    "1. parsed_news.tsv\n",
    "2. category2int.tsv\n",
    "3. embedding_weights.csv\n",
    "4. word2int.tsv\n",
    "##### Test\n",
    "1. parsed_news.tsv\n",
    "2. embedding_weights.csv\n",
    "3. word2int.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19830/19830 [00:00<00:00, 42158.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing eval data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2204/2204 [00:00<00:00, 55660.73it/s]\n",
      "100%|██████████| 4880/4880 [00:00<00:00, 2436981.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing eval data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7538/7538 [00:00<00:00, 61821.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load word-embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196017/2196017 [00:01<00:00, 1263508.77it/s]\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing/processing train news content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "  0%|          | 1/26740 [00:00<1:36:50,  4.60it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 26740/26740 [02:13<00:00, 200.29it/s]\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing/processing test news content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "100%|██████████| 18723/18723 [01:30<00:00, 207.46it/s]\n"
     ]
    }
   ],
   "source": [
    "prep_dataset.pre_processing_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
