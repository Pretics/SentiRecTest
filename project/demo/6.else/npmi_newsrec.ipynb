{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96481e3a",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 추천 시스템 구현해보기\n",
    "\n",
    "- (DC1) 실시간으로 사용자의 선호도 예측\n",
    "- (DC2) 자동화 방식의 뉴스 품질 측정\n",
    "- (DC3) 시의 적절한 주요 이슈 감지\n",
    "- (DC4) 확장성 있는 시스템 구조\n",
    "\n",
    "(S1) 후보 뉴스 기사 생성 단계 \n",
    "- CF-based Generation (DC1)\n",
    "- QE-based Generation (DC2)\n",
    "- SI(user)-based Generation (DC3)\n",
    "- SI(press)-based Generation (DC3)\n",
    "\n",
    "## 변수 설명\n",
    "\n",
    "사용자 ui가 최근 x 일 동안 소비한 각 뉴스 기사 vj, 소비하지 않은 뉴스 기사들 vk  \n",
    "P(vj)와 P(vk) 각각은 x일 동안 전체 사용자의 뉴스 소비 로그에서 뉴스 기사 vj, vk 각각의 소비 비율을 나타내고, P(vj, vk)은 vj, vk를 함께 소비한 비율을 나타냅니다.\n",
    "\n",
    "## 구현 과정\n",
    "\n",
    "1. behaviors.tsv 파일을 불러옵니다.\n",
    "2. history 순회를 돌며 P(vj), P(vk), P(vj, vk)에 대한 sparse matrix를 생성합니다.\n",
    "3. NPMI(vj, vk)를 생성합니다.\n",
    "\n",
    "### 조건\n",
    "matrix 크기를 줄이기 위해 항상 vj < vk로 취급합니다. vj == vk인 경우는 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea62659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook에서 import 해서 쓰는 모듈의 코드가 변경될 시, 변동 사항을 자동으로 반영해주는 기능 켜기\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf77174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nclud\\anaconda3\\envs\\newsrec\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "DATASET_SIZE = \"small\"\n",
    "PROJECT_DIR = path.abspath(path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATASET_DIR = path.join(PROJECT_DIR, \"data\", \"MIND\", DATASET_SIZE)\n",
    "\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "import torch\n",
    "\n",
    "from npmi_newsrec import NPMINewsrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8364e6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI 계산 전, history와 impressions에 대한 전처리를 진행합니다.\n",
      "1. news2int를 생성합니다.\n",
      "2. Dataframe의 history를 정리합니다.\n",
      "3. Dataframe의 impressions를 정리합니다.\n",
      "전처리를 완료했습니다.\n",
      "각 뉴스와 뉴스 쌍의 소비 비율을 계산하기 위해, history에서의 등장 횟수를 측정합니다.(중복 제외)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156965/156965 [02:01<00:00, 1289.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "측정한 횟수를 바탕으로 모든 뉴스 쌍의 NPMI 점수를 계산합니다.\n",
      "모든 뉴스 pair에 대한 npmi 점수를 저장한 dict를 생성합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11107456/11107456 [01:58<00:00, 93628.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성을 완료했습니다.\n",
      "batch를 생성합니다.\n",
      "train 데이터셋으로 계산한 뉴스 쌍의 NPMI 점수와 모든 사용자의 history를 기반으로, impression의 각 뉴스에 대한 최대 NPMI 점수를 찾습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156965/156965 [01:52<00:00, 1393.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch생성을 완료했습니다.\n"
     ]
    }
   ],
   "source": [
    "newsrec = NPMINewsrec(DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "882c7354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156965/156965 [00:02<00:00, 74277.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used 184/156965 samples.\n",
      "{'val_auc': tensor(0.5665), 'val_mrr': tensor(0.6393), 'val_ndcg@10': tensor(0.7082), 'val_ndcg@5': tensor(0.7082)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_result = newsrec.run_eval(0.9)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e4c464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156965/156965 [07:29<00:00, 349.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_auc': tensor(0.4997), 'val_mrr': tensor(0.2295), 'val_ndcg@10': tensor(0.2928), 'val_ndcg@5': tensor(0.2928)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_result_rand = newsrec.run_eval_random()\n",
    "print(test_result_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0062e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02623668871819973"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsrec.dataset.npmi_dict[(4, 36)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac151d7",
   "metadata": {},
   "source": [
    "## imp logs\n",
    "1. negative sampling\n",
    "2. torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d200940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 26): 0.2648686170578003\n",
      "(2, 36): 0.040272701531648636\n",
      "(4, 26): 0.02107863686978817\n",
      "(4, 36): 0.02623668871819973\n",
      "(15, 36): 0.15379023551940918\n",
      "(26, 30): 0.15547339618206024\n",
      "(26, 36): 0.00703651225194335\n",
      "(26, 39): 0.48407480120658875\n",
      "(29, 36): 0.10140280425548553\n",
      "(30, 36): 0.07632788270711899\n"
     ]
    }
   ],
   "source": [
    "newsrec.check_npmi(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6940b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(newsrec.dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e3bdc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 87, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"pairs_npmi\"].shape\n",
    "# batch_size, impr_num, history_num, pair_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_fixed_length(seqs: list[torch.Tensor], max_len: int, padding_value: int = 0):\n",
    "    # 먼저 자르기\n",
    "    clipped_seqs = [s[:max_len] for s in seqs]\n",
    "\n",
    "    # 패딩 적용\n",
    "    padded = pad_sequence(clipped_seqs, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    # 필요하면 오른쪽 끝까지 패딩 추가\n",
    "    if padded.shape[1] < max_len:\n",
    "        padded = torch.nn.functional.pad(\n",
    "            padded, (0, max_len - padded.shape[1]), value=padding_value\n",
    "        )\n",
    "\n",
    "    return padded\n",
    "\n",
    "history_idxs = [torch.tensor(history_idxs, dtype=torch.int32) for history_idxs in newsrec.dataset.train_behaviors_df[\"history_idxs\"]]\n",
    "impr_idxs = [torch.tensor(impr_idxs, dtype=torch.int32) for impr_idxs in newsrec.dataset.train_behaviors_df[\"impr_idxs\"]]\n",
    "history_idxs = pad_fixed_length(history_idxs, max_len=200)\n",
    "impr_idxs = pad_fixed_length(impr_idxs, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e211b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsrec.dataset.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76e74f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -1.0, -1.0, 0.1899547278881073, 0.17113995552062988, -1.0, -1.0, -1.0, -1.0, 0.2296501249074936, -1.0, -1.0, 0.2539237439632416, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "torch.Size([19])\n"
     ]
    }
   ],
   "source": [
    "print(newsrec.dataset.batch[0][\"scores\"].tolist())\n",
    "print(newsrec.dataset.batch[0][\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66aef724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"batch_MINDlarge.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"batch_idx\", \"score\", \"label\"])\n",
    "    \n",
    "    for batch_idx, entry in enumerate(newsrec.dataset.batch):\n",
    "        scores = entry[\"scores\"]\n",
    "        labels = entry[\"labels\"]\n",
    "\n",
    "        scores = entry[\"scores\"].tolist()\n",
    "        labels = entry[\"labels\"].tolist()\n",
    "        writer.writerow([batch_idx, scores, labels])\n",
    "\n",
    "        if batch_idx > 100:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ef6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 저장\n",
    "with open(\"batch_MINDlarge.pkl\", \"wb\") as f:\n",
    "    pickle.dump(newsrec.dataset.batch, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b41dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "with open(\"npmi_dict.pkl\", \"rb\") as f:\n",
    "    npmi_dict = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
