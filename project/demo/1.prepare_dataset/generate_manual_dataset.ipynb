{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual 버전 데이터셋 제작\n",
    "\n",
    "해당 코드는 설정한 갯수의 데이터를 기존의 데이터셋(demo/small/large)에서 랜덤하게 가져와 새로운 사이즈(manual - 이름 자유롭게 설정 가능)의 데이터셋을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "경로가 폴더를 나타낼 경우 Dir, 파일일 경우 Path로 명명\n",
    "\n",
    "size: 새로 만들 데이터셋의 크기 (자유롭게 설정 가능)\n",
    "original_size: 데이터를 가져올 기존 데이터셋의 크기 (demo, small, large 등 이미 있는 것)\n",
    "\"\"\"\n",
    "\n",
    "size = \"tiny\"\n",
    "original_size = \"demo\"\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n",
    "\n",
    "datasetDir = path.join(DATA_DIR, \"MIND\", size)\n",
    "datasetOriginalDir = path.join(DATA_DIR, \"MIND\", original_size)\n",
    "\n",
    "os.makedirs(path.join(datasetDir, \"train\"), exist_ok=True)\n",
    "os.makedirs(path.join(datasetDir, \"test\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    dataset_dir: str\n",
    "    dataset_original_dir: str\n",
    "    split_test_size: float\n",
    "    n_negative: int\n",
    "\n",
    "args = Args(\n",
    "    dataset_dir = datasetDir,\n",
    "    dataset_original_dir = datasetOriginalDir,\n",
    "    split_test_size = 0.1,\n",
    "    n_negative = 4\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "※중요: train_data_number는 2 이상으로 설정해야 합니다.\n",
    "나중에 데이터 전처리 과정에서 train_test_split()을 통해 train 데이터셋을 train/val로 쪼개기 때문입니다.\n",
    "\"\"\"\n",
    "train_data_number = 10\n",
    "test_data_number = 5\n",
    "random.seed(1234)\n",
    "\n",
    "def pick_random_integers(min: int, max: int, k: int):\n",
    "    if k > (max - min + 1):\n",
    "        raise ValueError(\"샘플 개수가 너무 많습니다.\")\n",
    "    elif k < 1:\n",
    "        raise ValueError(\"샘플 개수는 0보다 커야합니다.\")\n",
    "    return random.sample(range(min, max + 1), k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. manual Train/Test behaviors.tsv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_behaviors_dataset(\n",
    "        original_path: str,\n",
    "        out_path: str,\n",
    "        data_number: int\n",
    "    ):\n",
    "    # 원본 데이터셋을 불러옵니다.\n",
    "    with open(original_path, 'r') as original_behavior_file:\n",
    "        original_behaviors = original_behavior_file.readlines()\n",
    "\n",
    "    # 원본 데이터셋에서 뽑아올 샘플의 인덱스를 랜덤으로 생성합니다.\n",
    "    indexes = pick_random_integers(0, len(original_behaviors) - 1, data_number)\n",
    "\n",
    "    # 새로운 behaviors.tsv 파일을 생성합니다.\n",
    "    with open(out_path, 'w', newline='') as behavior_out_file:\n",
    "        behaviors_writer = csv.writer(behavior_out_file, delimiter='\\t')\n",
    "        for index in tqdm(indexes):\n",
    "            behavior_data = original_behaviors[index]\n",
    "            behaviors_writer.writerow(behavior_data.strip().split('\\t'))\n",
    "\n",
    "generate_behaviors_dataset(\n",
    "    path.join(args.dataset_original_dir, \"train\", \"behaviors.tsv\"),\n",
    "    path.join(args.dataset_dir, \"train\", \"behaviors.tsv\"),\n",
    "    train_data_number\n",
    ")\n",
    "generate_behaviors_dataset(\n",
    "    path.join(args.dataset_original_dir, \"test\", \"behaviors.tsv\"),\n",
    "    path.join(args.dataset_dir, \"test\", \"behaviors.tsv\"),\n",
    "    test_data_number\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. manual Train/Test news.tsv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16435.36it/s]\n",
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_news_dataset(\n",
    "        behaviors_path: str,\n",
    "        original_news_path: str,\n",
    "        out_path: str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    새로 생성하는 데이터셋의 behaviors.tsv에 포함된 뉴스 목록으로 news.tsv를 생성합니다.<br/>\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    `behaviors_path`: 새로 생성하는 데이터셋의 behaviors.tsv 경로입니다. <br/>\n",
    "    `original_news_path`: 원본 데이터셋의 news.tsv 경로입니다. <br/>\n",
    "    `out_path`: 새로 생성하는 데이터셋의 news.tsv를 저장할 경로입니다. <br/>\n",
    "    \"\"\"\n",
    "    # 생성한 behaviors.tsv 데이터셋을 불러옵니다.\n",
    "    with open(behaviors_path, 'r') as original_behavior_file:\n",
    "        behaviors = original_behavior_file.readlines()\n",
    "\n",
    "    # 원본 news.tsv 데이터셋을 불러옵니다.\n",
    "    news_columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entitles\", \"abstract_entities\"]\n",
    "    original_news_df = pd.read_csv(original_news_path, sep='\\t', header=None, names=news_columns, encoding='utf-8', index_col=\"news_id\")\n",
    "\n",
    "    # manual/train/behaviors.tsv로 선별한 모든 샘플에 포함된 뉴스 목록을 news.tsv에 저장하기 위한 전처리 과정을 시작합니다.\n",
    "    news_collection = set()\n",
    "    for behavior_data in tqdm(behaviors):\n",
    "        imp_id, user_id, time, history, impressions = behavior_data.strip().split('\\t')\n",
    "        # NewsID가 들어 있는 history, impressions에서 ID만 빼옵니다.\n",
    "        history = history.split(' ')\n",
    "        impressions = [s.split('-')[0] for s in impressions.split(' ')]\n",
    "        # 집합에 추가해서 중복을 제거합니다.\n",
    "        # 가끔 history나 impressions가 없으면 ['']이 저장되는데\n",
    "        # 이걸 집합에 추가하면 Dataframe 생성시 문제가 생기므로\n",
    "        # 조건문으로 걸러줍니다.\n",
    "        if history[0] != '':\n",
    "            news_collection.update(history)\n",
    "        if impressions[0] != '':\n",
    "            news_collection.update(impressions)\n",
    "\n",
    "    # 중복 없이 뽑아낸 모든 뉴스ID의 데이터를 선별합니다.\n",
    "    news_df = original_news_df.loc[list(news_collection)]\n",
    "\n",
    "    # 선별한 데이터를 news.tsv에 저장합니다.\n",
    "    news_df.to_csv(out_path, sep='\\t', header=None, encoding='utf-8')\n",
    "    return news_df\n",
    "\n",
    "train_news_df = generate_news_dataset(\n",
    "    path.join(args.dataset_dir, \"train\", \"behaviors.tsv\"),\n",
    "    path.join(args.dataset_original_dir, \"train\", \"news.tsv\"),\n",
    "    path.join(args.dataset_dir, \"train\", \"news.tsv\")\n",
    ")\n",
    "test_news_df = generate_news_dataset(\n",
    "    path.join(args.dataset_dir, \"test\", \"behaviors.tsv\"),\n",
    "    path.join(args.dataset_original_dir, \"test\", \"news.tsv\"),\n",
    "    path.join(args.dataset_dir, \"test\", \"news.tsv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기타 테스트 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(938, 7)\n",
      "(443, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train_news_df.shape)\n",
    "print(test_news_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
